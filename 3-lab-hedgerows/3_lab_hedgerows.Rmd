---
title: Manipulating and visualizing data part 2
output: pdf_document
author: L. Ponisio
---

# Do the bees at hedgerows and weedy field margins (controls) have the same distribution of traits? 

 Our goal is to reproduce the analyses in Ponisio et al. 2016, Onâ€farm habitat restoration counters biotic homogenization in intensively managed agriculture, [Ponisio et al. 2016](https://doi.org/10.1111/gcb.13117)

Specifically, we are going to reproduce Figure 4 panels a-b, where we examine whether the bees at hedgerows (restored habitat) and controls (weedy field margins that would have a hedgerow) are random subset of the species pool.

## Conservation/ecology Topics 

> - Become familiar with different metrics of biodiversity, including trait diversity.  
> - Become familiar with the concept of biotic homogenization. 

## Computational Topics

> - Join data (new)
> - Make histograms (new)
> - Subsetting, reshaping data using dplyr (more practice)
> - Use logical indexing to subset data (more practice)
> - Write functions (more practice)
> - Write for loops (more practice)

## Statistical Topics

> - AB testing, simulating the null hypothesis 
> - Calculate empirical p-values
> - Hypothesis testings: reject or accept our null hypothesis

-------------------------------

```{r}
# Load necessary libraries in R
library(ggplot2)
library(dplyr, warn.conflicts = FALSE)
library(readr)
library(tidyr)

# For reproducibility purposes
set.seed(123)
```

```{r}
## read in the specimen data 
spec <- read.csv("specimens-complete.csv")

## explore the columns
str(spec)
```

Each row is a uniquely identified insect specimen and all of it's attributes. If we explore the species ID-related columns in more depth, we can see that there are many different types of insects in addition to bees. There are also some specimens that have never been IDed (missed data is a blank cell in these case). 

```{r}
unique(spec$Order)
unique(spec$Family)
#sort(unique(spec$GenusSpecies))
```

# Lab 3 Part 1: Practicing the data subsetting tools from dplyr

a. Thinking back to the dplyr tools you learned last week, subset the data to only bees. The families of bees are "Halictidae", "Megachilidae", "Apidae", "Colletidae" "Andrenidae" and "Melittidae". In addition, drop all rows without anything in the 'Species' column, otherwise we could end up with specimens only identified to Genus.  

Keep the data object named spec (i.e., don't create a new name for the subsetted data.)  

HINT: %in% maybe useful, or remember how to string together Booleans.  


```{r}
spec <- spec %>%
  filter(Family %in% c("Halictidae", "Megachilidae", "Apidae", "Colletidae", "Andrenidae", "Melittidae") & Species != "")

## The dimensions of the subsetted data should be 99097 by 40
dim(spec)
```

Now take a look at the site statuses, or the site "treatments",

```{r}
unique(spec$SiteStatus)
unique(spec$Sex)
unique(spec$NetPan)
```

This data include many more site treatments than we need to reproduce the publication because it includes all of the sites ever surveyed in this landscape. We don't really care about the restored sites (that is the year the hedgerow was planted so they are between a hedgerow and a control). We don't have many "natural" sites  (we couldn't find any). Forbs are hedgerows with additional forb plantings, also not what we need.  

b. Subset only to mature hedgerows and controls (weedy field margins). We also only care about the "hand-netted" data (the column 'NetPan') and female bees ('Sex') (male bees don't collect pollen). Do all of this subsetting in one call to filter (you will need to combine your comparisons using & or |. 

Similar to before keep the name of the data the same. The publication also includes the maturing sites, but we are going to make our lives a little easier by keeping our analysis to two treatments. 

```{r}
spec <- spec %>% filter(NetPan == "net" & Sex == "f" & SiteStatus %in% c("mature", "control"))
head(spec)
## The dimensions of the subsetted data should be 8810 by 40.
dim(spec)
```

c. Lastly, drop all the random columns we don't care about. All we want to keep are "UniqueID", "GenusSpecies", "Site", "SiteStatus" and "Year". 

```{r}
## let's drop all the random columns we don't care about
spec <- spec %>% select("UniqueID", "GenusSpecies", "Site", "SiteStatus", "Year")

str(spec)
```

# Lab part 2: Joining data 

Next, we want to add in species traits. 

```{r}
## load the traits table, this includes traits for all the bees
traits <- read.csv("bee.csv")
str(traits)
```

We will often find ourselves in a situation where we want to combine two datasets by some shared column. In dyplyr, mutating joins add columns from y to x, matching observations based on the keys. There are four mutating joins: the inner join, and the three outer joins.

> - Inner join: An inner_join() only keeps observations from x that have a matching key in y. The most important property of an inner join is that unmatched rows in either input are not included in the result, so data is dropped. 

```{r}
df1 <- data.frame(x = c(1:4, NA), y = 2)
df2 <- data.frame(x = c(1:2), z = 3)
df1
df2

inner <- inner_join(df1, df2)
inner
```

> - Outer joins: The three outer joins keep observations that appear in at least one of the data frames:

a. A left_join() keeps all observations in x.

```{r}
left <- left_join(df1, df2)
left
```

b. A right_join() keeps all observations in y.

```{r}
right <- right_join(df1, df2)
right
```

c. A full_join() keeps all observations in x and y.

```{r}
full <- full_join(df1, df2)
full
```

Class discussion: What type of join do we want to use to to combine the individual bee data with their traits? 

# Lab Question 2

a. Join the trait data to the individual bee data. Before joining the data, subset the trait to only the bee IDs ('GenusSpecies'), body size ('MeanITD'), floral specialization ('d').  Call the joined data spec.traits
```{r}
#spec
#traits
```

```{r}
traits <- traits %>% select("GenusSpecies", "MeanITD", "d")
spec.traits <- left_join(spec, traits, by="GenusSpecies")
#spec.traits
## The dimensions should be 8810    7
dim(spec.traits)
```

b. How many specimens are missing body size data? 
HINT: the function 'is.na()' may be useful.
EXTRA HINT: Remember the TRUEs are counted as 1s in R.

```{r}
sum(is.na(spec.traits$MeanITD))
```

c. What are the unique species names missing body size data. 
```{r}
missing_body_size <- spec.traits %>% filter(is.na(MeanITD))
unique_spec_no_size <- (unique(missing_body_size$GenusSpecies))
unique_spec_no_size
```
d. These were relatively rare species that didn't have enough individuals to accurately estimate body size. Since we cannot do much with them, go ahead and drop them. Keep the data named spec.traits. You can use the tools we have learned in dyplyr, or just clever indexing. 

HINT: There are several ways you can drop the rows without body size data, try to avoid hard coding by copy pasting the names above. What if you added new data and wanted to re-run the code? You might miss some species if you remove them "by hand". 
EXTRA HINT: Remember that ! reverses TRUEs and FALSEs (so TRUE because FALSE and vice versa).

EXTRA EXTRA HINT: If you decide to use indexing, remember that when we index a 2D object we need to specify the indexes of the rows and columns. 
```{r}
spec.traits <- spec.traits %>% filter(!(GenusSpecies %in% unique_spec_no_size))
## The dimensions should be 8634 7
#spec.traits
dim(spec.traits)
```

# Lab Question 3

a. Use 'group_by' and 'summarize' to calculate the average body size and specialization for each site, year, site status combination. Create a new object called sys.traits

```{r}
sys.traits <- spec.traits %>%
  group_by(Site, Year, SiteStatus) %>%
  summarize(
    average_body_size = mean(MeanITD, na.rm = TRUE),  
    average_specialization = mean(d, na.rm = TRUE)  
  )

#sys.traits
```

b. Visualize this data with a histograms for each trait.
HINT: 'geom_histogram()' is the function that makes histograms, and it only needs an x aesthetic. 
```{r}
## body size
ggplot(sys.traits, aes(x = average_body_size)) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of Average Body Size",
       x = "Average Body Size",
       y = "Frequency")
```


```{r}
## Specialization
ggplot(sys.traits, aes(x = average_specialization)) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of Average Specialization",
       x = "Average Specialization",
       y = "Frequency")
```
c. Describe what you see.
-- ANSWER: Average body size is centered around 1.2 and has a right skewed distribution, meaning there is more spread among higher body sizes. Less sites have higher average body sizes observed in a given year for either site status. 
Average specialization is centered somewhere around <0.15, also with a right skewed distribution. Considering that both distributions have a similar shape, we might conclude that body size is tied to specialization (body size indicates species, which tends to correlate to a certain plant species).

# Lab Part 3: Do the bees at hedgerows and weedy field margins (controls) have the same distribution of traits? 

- Null hypothesis: Mature hedgerows and weedy field margins (controls) support bees with similar traits, so the average trait values at hedgerows and controls is similar. 
- Alternative hypothesis: The traits at mature hedgerows and controls are not the same. 
 a) Mature hedgerows have large bees and more specialized bees than would be expected if we were sampling randomly from the regional species pool. 
 b) Controls  have smaller bees and more generalized bees than would be expected if we were sampling randomly from the regional species pool.


In the study, we tested the hypothesis above by creating a null distribution of average trait values by shuffling individuals across sites within a year. This enabled us to constrain species abundance and richness at a site while changing the identity of the individuals. Creating this type of custom null model is a bit out of the scope of what we know how to do at this point in the class, so instead we will simplify our problem by shuffling the treatment columns (mature hedgerow vs. control) across sites within a year. We will then take the average trait value for the randomized data to calculate the null expectation of controls b

## Step 0: Let's first try to do our randomization with 1 year of data then work out way up. 
```{r}
## Let's pick 2013, it was a nice year.
yr_2013 <- spec.traits %>% filter(Year == 2013)
#yr_2013
```
## Step 1. Shuffle the treatment column so that the treatment each 
```{r}
shuffled_col <- sample(yr_2013$SiteStatus, size=nrow(yr_2013), replace=FALSE)
```

# Step 2: Add the shuffled column back on to the table
```{r}
yr_2013 <- yr_2013 %>% mutate(shuffled_status = shuffled_col)
#yr_2013
```
## Step 3: Take the mean of the simulated data.
```{r}
sim_control_mean <- (yr_2013 %>% filter(shuffled_status == "control") %>% select(Year, SiteStatus, MeanITD, d))

sim_mature_mean <- (yr_2013 %>% filter(shuffled_status == "mature") %>% select(Year, SiteStatus, MeanITD, d))

bound <- rbind(sim_control_mean, sim_mature_mean)
```

# Lab Question 4

a. Combine Steps 1-3 into a function: Now that we have worked through all the steps with one year, write a function that takes yr and sys.traits. Don't forget to add a nice annotation in the first few line describing what the function does. Call your function 'randomizeTraits'

```{r}
# This function takes a year and a traits dataset as input,
# shuffles the SiteStatus for that year's data, and computes the mean trait values 
# for the control and mature treatments in the shuffled data.

randomizeTraits <- function(yr, traits) {
  
  # Step 0: Filter the data for the specified year
  yr_data <- traits %>% filter(Year == yr)
  
  # Step 1: Shuffle the SiteStatus column
  shuffled_col <- sample(yr_data$SiteStatus, size = nrow(yr_data), replace = FALSE)
  
  # Step 2: Add the shuffled column back to the dataset
  yr_data$shuffled_status <- shuffled_col

  # Step 3: Compute the mean trait values for the control and mature groups
  sim_control_mean <- yr_data %>% 
    filter(shuffled_status == "control") %>% 
    summarise(mean_size = mean(average_body_size, na.rm = TRUE),
              mean_specialization = mean(average_specialization, na.rm = TRUE),
              status = "control") # Add status column for labeling
  
  sim_mature_mean <- yr_data %>% 
    filter(shuffled_status == "mature") %>% 
    summarise(mean_size = mean(average_body_size, na.rm = TRUE),
              mean_specialization = mean(average_specialization, na.rm = TRUE),
              status = "mature") # Add status column for labeling
  
  # Combine the results for both control and mature groups
  result <- rbind(sim_control_mean, sim_mature_mean)
  
  # Add the year column to indicate which year this result is for
  result$Year <- yr
  
  return(result)
}



```

b.  Test out your function on a 2013. Run your function a few times. Do the mean values for the traits change? Why? 
```{r}
randomizeTraits(2013, sys.traits)
```
-- ANSWER: Yes, the mean values for the traits change each time you run this, because we are re-simulating each time. So, we would expect both means to be quite similar, but due to the randomness of shuffling the labels we can get two means that may seem extremely different. 

# Lab Question 5
Now we want to use our function on each year of data and save our results. The perfect situation for a for loop! 

a. We will want to loop over the different years in the data. Start by creating a vector of the unique year values, call it 'years'. It would be nice to sort that vector as well so that the years are in order. 
```{r}

years <- sort(unique(sys.traits$Year))
```

b. We will do this together in class, but first try to think it through.

The next step was a bit tricky. Unlike before in class (lab 1),  where we saved the calculations from our function into a vector, we have a dataframe  I found that the easiest way to get the column names right was to start to the first year of data, then rbind() (row bind) that to the previous year. Call your randomized data random.traits

HINT: If you do as I suggested above, you will not want to include year one in your for loop. If you have a vector x, x[-1] will drop the first element of that vector.


```{r}
## Start with the first year to set up the table
random.traits <- randomizeTraits(yr=years[1], sys.traits)
for (yr in years[-1]) { # -1 drops the first element
  new.year <- randomizeTraits(yr, sys.traits)
  random.traits <- rbind(random.traits, new.year) # add the new row each time
}
#random.traits

```

c. Take the mean body size and specialization for each site status across years of that simulated full dataset, call it mean.years
```{r}
mean.years <- random.traits %>%
  group_by(status) %>%
  summarize(
    mean_body_size = mean(mean_size, na.rm = TRUE),
    mean_specialization = mean(mean_specialization, na.rm = TRUE)
  )
mean.years
```

d. Now combine all the steps above into a single function called repRandomComm that:
- shuffles year 1 using the randomizeTraits function
- loops over the remaining years, row binding the data to prior year each time 
- takes the mean of the traits across years, and returns that value. 

It should have two arguments years and sys.traits

NOTE: It isn't A+ to have argument names be the same as objects in your Global Environment (can you think of why?), but sometimes argument names can just too silly if you keep trying to tweak them. Let's just be kinda A- with our naming to avoid confusion. 

```{r}
repRandomComm <- function(year_data, traits_data) {
  # Step 1: Randomize year 1 using the randomizeTraits function
  randomized_data <- randomizeTraits(year_data[[1]], traits_data)
  
  # Step 2: Loop over remaining years, row-binding each new randomized year to the cumulative data
  for (i in 2:length(year_data)) {
    yearly_data <- randomizeTraits(year_data[[i]], traits_data)
    randomized_data <- bind_rows(randomized_data, yearly_data)
  }
  
  # Step 3: Calculate the mean body size and specialization across years for each SiteStatus
  mean_across_years <- randomized_data %>%
    group_by(status) %>%
    summarize(
      mean_body_size = mean(mean_size, na.rm = TRUE),
      mean_specialization = mean(mean_specialization, na.rm = TRUE)
    )
  
  return(mean_across_years)
}

```

Try out your function! 

```{r, message=FALSE}
repRandomComm(years, sys.traits)
```
d. Last step! Run your randomization 100 times using a for loop. Similar to our above for loop, it was helpful to run the randomization once to get the format of the dataframe correct, then just rbind the rest of the iterations onto that. 
```{r, message=FALSE}
randomized.comms <- data.frame()

for (i in 1:100) {
  randomized.comms <- rbind(randomized.comms, repRandomComm(years, sys.traits))
}

## The dimensions should be 200 3 (two rows are created each iteration)
dim(randomized.comms)
```

# Lab Part 4: Interpreting the results
To determine whether our randomized communities have similar trait values as our observed communities, we will calculate an empirical p-value. 

Empirical p-value is calculated as the number of permutation tests with a 'more extreme' (larger or smaller, depending on the null hypothesis) observed test statistic. 

In our case, our test statistic is the mean body size (MeanITD) and mean specialization (d). 

We will start by calculating our observed test statistic for both traits. 

```{r}
 observed_means <- sys.traits %>%
  group_by(SiteStatus) %>%
  summarize(
    observed_mean_body_size = mean(average_body_size, na.rm = TRUE),
    observed_mean_specialization = mean(average_specialization, na.rm = TRUE)
  )

observed_means
observed_means$observed_mean_body_size[2]
```

To visualize our observed data in relation to our randomized data, we will plot a histogram of our simulated data and add a vertical line for our observed data. 

```{r}
ggplot(randomized.comms, aes(x = mean_body_size)) +
  geom_histogram(fill = "blue", color = "black", bins = 30) +
  geom_vline(aes(xintercept = observed_means$observed_mean_body_size[1], color = "Control"),
             linetype = "dashed", size = 1) + 
  geom_vline(aes(xintercept = observed_means$observed_mean_body_size[2], color = "Mature"),
             linetype = "dashed", size = 1) +
  labs(title = "Histogram of Simulated Mean Body Size",
       x = "Mean Body Size",
       y = "Frequency") +
  scale_color_manual(name = "Treatment",
                     values = c("Control" = "red", "Mature" = "pink"))

```
Our observed data looks quite different than our randomized data! 

If our null hypothesis that bees at mature hedgerows and controls have similar traits is true what is the probability we get bees as small as we did at the controls? Let's calculate that probability. We need to calculate how many times we got values equal to or more extreme than our observed value when we randomized the treatments (mature hedgerow, control) in the data.

Remember that we can sum the TRUEs. 
```{r}
more_extreme_control <- (sum(randomized.comms$mean_body_size <= observed_means$observed_mean_body_size[1])) / nrow(randomized.comms)

more_extreme_control
```

This is our empirical p-value. That is a pretty low probability that we would get our observed statistic if the null hypothesis (the traits in mature and control communities the same) was true. 

We generally reject our null hypothesis if our p-value is less than 0.05.  We can therefore reject our null hypothesis. 

Now for the mature communities: 

```{r}
more_extreme_mature <- (sum(randomized.comms$mean_body_size >= observed_means$observed_mean_body_size[2]))/ nrow(randomized.comms)

more_extreme_mature

```

Again, that is a pretty low probability that we would get our observed statistic if the null hypothesis (the traits in mature and control communities the same) was true. Our p value is again less than 0.03. We can therefore reject our null hypothesis. 

# Lab question 6

Follow the same steps for the specialization (d)
a. Make a histogram with the randomized and observed values.
```{r}

ggplot(randomized.comms, aes(x = mean_specialization)) +
  geom_histogram(fill = "blue", color = "black", bins = 30) +
  geom_vline(aes(xintercept = observed_means$observed_mean_specialization[2], color = "Control"),
             linetype = "dashed", size = 1) + 
  geom_vline(aes(xintercept = observed_means$observed_mean_specialization[1], color = "Mature"),
             linetype = "dashed", size = 1) +
  labs(title = "Histogram of Simulated Mean Specialization",
       x = "Mean Specialization",
       y = "Frequency") +
  scale_color_manual(name = "Treatment",
                     values = c("Control" = "red", "Mature" = "pink"))
```

b.  What is the probability that bees at controls are as generalized (so small values of d) as what we observed, given then null hypothesis is true?
```{r}
more_extreme_d_control <- (sum(randomized.comms$mean_specialization <= observed_means$observed_mean_specialization[1])) / nrow(randomized.comms)

more_extreme_d_control
```

c. Do we accept or reject our null hypothesis? 
-- ANSWER: We reject our null hypothesis since the probability of getting an observed statistic as extreme or more extreme is approximately 0.

d.  What is the probability that bees at mature hedgerows are as specialized (so large values of d) as what we observed, given then null hypothesis is true?
```{r}
more_extreme_d_mature <- (sum(randomized.comms$mean_specialization <= observed_means$observed_mean_specialization[1])) / nrow(randomized.comms)

more_extreme_d_mature
```
f. Do we accept or reject our null hypothesis? 

-- ANSWER: Similarly, we reject our null hypothesis.

With these analyses we have approximated the analysis of Ponisio et al. 2016. Congratulations :) 